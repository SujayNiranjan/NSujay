---
layout: default
title: MappingNYC
description: Multimodal Mapping for Autonomous Urban Navigation
---

<h1>MappingNYC</h1>

<h2><strong>Project Overview</strong></h2>
<p>MappingNYC is a multimodal urban mapping project aimed at enhancing the capabilities of autonomous vehicles in dense urban areas like New York City.</p>

<h2><strong>Technologies Used</strong></h2>
<ul>
  <li><strong>Hardware</strong>: Dual LiDARs, Dual 360-degree cameras, GPS.</li>
  <li><strong>Software</strong>: ROS, Fast-LIO, GTSAM, Visual Place Recognition, SLAM techniques like PIN-SLAM, Scan Context.</li>
</ul>

<h2><strong>Key Contributions</strong></h2>
<ol>
  <li>Developed a robust sensor mount for collecting multimodal data in urban environments.</li>
  <li>Implemented <strong>Fast-LIO</strong> for real-time odometry and <strong>Visual Place Recognition</strong> for loop closure.</li>
  <li>Created scalable maps using <strong>graph-based optimization</strong> with <strong>GTSAM</strong>.</li>
  <li>Ensured session merging across mapping routes using <strong>AutoMerge</strong>.</li>
</ol>

<h2><strong>Outcomes</strong></h2>
<p>The project has resulted in:</p>
<ul>
  <li>High-resolution maps of NYC, supporting the research community.</li>
  <li>A comprehensive dataset available for training and testing autonomous navigation systems.</li>
</ul>

<h2><strong>Media</strong></h2>
<p><img src="mappingnyc-image1.jpg" alt="MappingNYC Sample Map" />  
<em>Figure: Example of a multimodal map generated for NYC.</em></p>

<p><img src="mappingnyc-image2.jpg" alt="MappingNYC Sensor Mount" />  
<em>Figure: Sensor mount prototype.</em></p>

<p><a href="../../">Return to Projects</a></p>

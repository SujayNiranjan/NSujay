---
layout: default
title: MappingNYC
description: Multimodal Mapping for Autonomous Urban Navigation
---

# MappingNYC

## **Project Overview**
MappingNYC is a multimodal urban mapping project aimed at enhancing the capabilities of autonomous vehicles in dense urban areas like New York City. 

## **Technologies Used**
- **Hardware**: Dual LiDARs, Dual 360-degree cameras, GPS.
- **Software**: ROS, Fast-LIO, GTSAM, Visual Place Recognition, SLAM techniques like PIN-SLAM, Scan Context.

## **Key Contributions**
1. Developed a robust sensor mount for collecting multimodal data in urban environments.
2. Implemented **Fast-LIO** for real-time odometry and **Visual Place Recognition** for loop closure.
3. Created scalable maps using **graph-based optimization** with **GTSAM**.
4. Ensured session merging across mapping routes using **AutoMerge**.

## **Outcomes**
The project has resulted in:
- High-resolution maps of NYC, supporting the research community.
- A comprehensive dataset available for training and testing autonomous navigation systems.

## **Media**
![MappingNYC Sample Map](mappingnyc-image1.jpg)  
*Figure: Example of a multimodal map generated for NYC.*

![MappingNYC Sensor Mount](mappingnyc-image2.jpg)  
*Figure: Sensor mount prototype.*

[Return to Projects](../../)
